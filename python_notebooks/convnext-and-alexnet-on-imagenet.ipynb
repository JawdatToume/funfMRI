{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "541d6d64",
   "metadata": {
    "papermill": {
     "duration": 0.008753,
     "end_time": "2023-11-13T00:34:11.527452",
     "exception": false,
     "start_time": "2023-11-13T00:34:11.518699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Some imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adceef3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-12T22:21:06.002402Z",
     "iopub.status.busy": "2023-11-12T22:21:06.001750Z",
     "iopub.status.idle": "2023-11-12T22:21:06.010485Z",
     "shell.execute_reply": "2023-11-12T22:21:06.008938Z",
     "shell.execute_reply.started": "2023-11-12T22:21:06.002366Z"
    },
    "papermill": {
     "duration": 0.008028,
     "end_time": "2023-11-13T00:34:11.543941",
     "exception": false,
     "start_time": "2023-11-13T00:34:11.535913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ec2b259",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-11-13T00:34:11.563237Z",
     "iopub.status.busy": "2023-11-13T00:34:11.562393Z",
     "iopub.status.idle": "2023-11-13T00:34:45.077863Z",
     "shell.execute_reply": "2023-11-13T00:34:45.076529Z"
    },
    "papermill": {
     "duration": 33.528053,
     "end_time": "2023-11-13T00:34:45.080414",
     "exception": false,
     "start_time": "2023-11-13T00:34:11.552361",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensordict\r\n",
      "  Downloading tensordict-0.2.1-cp310-cp310-manylinux1_x86_64.whl (986 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m986.5/986.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from tensordict) (2.0.0+cpu)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensordict) (1.23.5)\r\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from tensordict) (2.2.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->tensordict) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->tensordict) (4.6.3)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->tensordict) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->tensordict) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->tensordict) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->tensordict) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->tensordict) (1.3.0)\r\n",
      "Installing collected packages: tensordict\r\n",
      "Successfully installed tensordict-0.2.1\r\n",
      "Collecting torchviz\r\n",
      "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchviz) (2.0.0+cpu)\r\n",
      "Requirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from torchviz) (0.20.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (4.6.3)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchviz) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchviz) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchviz) (1.3.0)\r\n",
      "Building wheels for collected packages: torchviz\r\n",
      "  Building wheel for torchviz (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=f409643b869e66203266ce85799f03d2d970df45eca722b7c76d40e648579eab\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\r\n",
      "Successfully built torchviz\r\n",
      "Installing collected packages: torchviz\r\n",
      "Successfully installed torchviz-0.0.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensordict\n",
    "!pip install torchviz\n",
    "from torchvision.models import convnext_tiny,ConvNeXt_Tiny_Weights\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80d68b",
   "metadata": {
    "papermill": {
     "duration": 0.009359,
     "end_time": "2023-11-13T00:34:45.099913",
     "exception": false,
     "start_time": "2023-11-13T00:34:45.090554",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8559d29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:34:45.121987Z",
     "iopub.status.busy": "2023-11-13T00:34:45.120993Z",
     "iopub.status.idle": "2023-11-13T00:34:47.356133Z",
     "shell.execute_reply": "2023-11-13T00:34:47.355183Z"
    },
    "papermill": {
     "duration": 2.248713,
     "end_time": "2023-11-13T00:34:47.358828",
     "exception": false,
     "start_time": "2023-11-13T00:34:45.110115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n",
      "100%|██████████| 109M/109M [00:00<00:00, 188MB/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "African elephant : 35.99937856197357 % \n"
     ]
    }
   ],
   "source": [
    "# Testing convnext for class prediction on imagenet\n",
    "convnext_weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1\n",
    "model=convnext_tiny(weights=convnext_weights)\n",
    "#print(model)\n",
    "model.eval()\n",
    "\n",
    "preprocess=convnext_weights.transforms()\n",
    "img=read_image(\"/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/test/ILSVRC2012_test_00000046.JPEG\")\n",
    "batch=preprocess(img).unsqueeze(0)\n",
    "\n",
    "prediction=model(batch).squeeze(0).softmax(0)\n",
    "class_id= prediction.argmax().item()\n",
    "score=prediction[class_id].item()\n",
    "category_name=convnext_weights.meta[\"categories\"][class_id]\n",
    "print(f\"{category_name} : {score*100} % \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0195877d",
   "metadata": {
    "papermill": {
     "duration": 0.010151,
     "end_time": "2023-11-13T00:34:47.383591",
     "exception": false,
     "start_time": "2023-11-13T00:34:47.373440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Remove the top layer and Extracting features vectors from the convnext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b61c16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:34:47.407255Z",
     "iopub.status.busy": "2023-11-13T00:34:47.406241Z",
     "iopub.status.idle": "2023-11-13T00:34:59.808067Z",
     "shell.execute_reply": "2023-11-13T00:34:59.806257Z"
    },
    "papermill": {
     "duration": 12.416709,
     "end_time": "2023-11-13T00:34:59.810533",
     "exception": false,
     "start_time": "2023-11-13T00:34:47.393824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################SHOW THE MODEL NODES##############\n",
      "Train node 0 : x\n",
      "Train node 1 : features.0\n",
      "Train node 2 : features.1.0.features_1_0_layer_scale\n",
      "Train node 3 : features.1.0.block.0\n",
      "Train node 4 : features.1.0.block.1\n",
      "Train node 5 : features.1.0.block.2\n",
      "Train node 6 : features.1.0.block.3\n",
      "Train node 7 : features.1.0.block.4\n",
      "Train node 8 : features.1.0.block.5\n",
      "Train node 9 : features.1.0.block.6\n",
      "Train node 10 : features.1.0.mul\n",
      "Train node 11 : features.1.0.stochastic_depth\n",
      "Train node 12 : features.1.0.add\n",
      "Train node 13 : features.1.1.features_1_1_layer_scale\n",
      "Train node 14 : features.1.1.block.0\n",
      "Train node 15 : features.1.1.block.1\n",
      "Train node 16 : features.1.1.block.2\n",
      "Train node 17 : features.1.1.block.3\n",
      "Train node 18 : features.1.1.block.4\n",
      "Train node 19 : features.1.1.block.5\n",
      "Train node 20 : features.1.1.block.6\n",
      "Train node 21 : features.1.1.mul\n",
      "Train node 22 : features.1.1.stochastic_depth\n",
      "Train node 23 : features.1.1.add\n",
      "Train node 24 : features.1.2.features_1_2_layer_scale\n",
      "Train node 25 : features.1.2.block.0\n",
      "Train node 26 : features.1.2.block.1\n",
      "Train node 27 : features.1.2.block.2\n",
      "Train node 28 : features.1.2.block.3\n",
      "Train node 29 : features.1.2.block.4\n",
      "Train node 30 : features.1.2.block.5\n",
      "Train node 31 : features.1.2.block.6\n",
      "Train node 32 : features.1.2.mul\n",
      "Train node 33 : features.1.2.stochastic_depth\n",
      "Train node 34 : features.1.2.add\n",
      "Train node 35 : features.2.0.permute\n",
      "Train node 36 : features.2.0.features_2_0_weight\n",
      "Train node 37 : features.2.0.features_2_0_bias\n",
      "Train node 38 : features.2.0.layer_norm\n",
      "Train node 39 : features.2.0.permute_1\n",
      "Train node 40 : features.2.1\n",
      "Train node 41 : features.3.0.features_3_0_layer_scale\n",
      "Train node 42 : features.3.0.block.0\n",
      "Train node 43 : features.3.0.block.1\n",
      "Train node 44 : features.3.0.block.2\n",
      "Train node 45 : features.3.0.block.3\n",
      "Train node 46 : features.3.0.block.4\n",
      "Train node 47 : features.3.0.block.5\n",
      "Train node 48 : features.3.0.block.6\n",
      "Train node 49 : features.3.0.mul\n",
      "Train node 50 : features.3.0.stochastic_depth\n",
      "Train node 51 : features.3.0.add\n",
      "Train node 52 : features.3.1.features_3_1_layer_scale\n",
      "Train node 53 : features.3.1.block.0\n",
      "Train node 54 : features.3.1.block.1\n",
      "Train node 55 : features.3.1.block.2\n",
      "Train node 56 : features.3.1.block.3\n",
      "Train node 57 : features.3.1.block.4\n",
      "Train node 58 : features.3.1.block.5\n",
      "Train node 59 : features.3.1.block.6\n",
      "Train node 60 : features.3.1.mul\n",
      "Train node 61 : features.3.1.stochastic_depth\n",
      "Train node 62 : features.3.1.add\n",
      "Train node 63 : features.3.2.features_3_2_layer_scale\n",
      "Train node 64 : features.3.2.block.0\n",
      "Train node 65 : features.3.2.block.1\n",
      "Train node 66 : features.3.2.block.2\n",
      "Train node 67 : features.3.2.block.3\n",
      "Train node 68 : features.3.2.block.4\n",
      "Train node 69 : features.3.2.block.5\n",
      "Train node 70 : features.3.2.block.6\n",
      "Train node 71 : features.3.2.mul\n",
      "Train node 72 : features.3.2.stochastic_depth\n",
      "Train node 73 : features.3.2.add\n",
      "Train node 74 : features.4.0.permute\n",
      "Train node 75 : features.4.0.features_4_0_weight\n",
      "Train node 76 : features.4.0.features_4_0_bias\n",
      "Train node 77 : features.4.0.layer_norm\n",
      "Train node 78 : features.4.0.permute_1\n",
      "Train node 79 : features.4.1\n",
      "Train node 80 : features.5.0.features_5_0_layer_scale\n",
      "Train node 81 : features.5.0.block.0\n",
      "Train node 82 : features.5.0.block.1\n",
      "Train node 83 : features.5.0.block.2\n",
      "Train node 84 : features.5.0.block.3\n",
      "Train node 85 : features.5.0.block.4\n",
      "Train node 86 : features.5.0.block.5\n",
      "Train node 87 : features.5.0.block.6\n",
      "Train node 88 : features.5.0.mul\n",
      "Train node 89 : features.5.0.stochastic_depth\n",
      "Train node 90 : features.5.0.add\n",
      "Train node 91 : features.5.1.features_5_1_layer_scale\n",
      "Train node 92 : features.5.1.block.0\n",
      "Train node 93 : features.5.1.block.1\n",
      "Train node 94 : features.5.1.block.2\n",
      "Train node 95 : features.5.1.block.3\n",
      "Train node 96 : features.5.1.block.4\n",
      "Train node 97 : features.5.1.block.5\n",
      "Train node 98 : features.5.1.block.6\n",
      "Train node 99 : features.5.1.mul\n",
      "Train node 100 : features.5.1.stochastic_depth\n",
      "Train node 101 : features.5.1.add\n",
      "Train node 102 : features.5.2.features_5_2_layer_scale\n",
      "Train node 103 : features.5.2.block.0\n",
      "Train node 104 : features.5.2.block.1\n",
      "Train node 105 : features.5.2.block.2\n",
      "Train node 106 : features.5.2.block.3\n",
      "Train node 107 : features.5.2.block.4\n",
      "Train node 108 : features.5.2.block.5\n",
      "Train node 109 : features.5.2.block.6\n",
      "Train node 110 : features.5.2.mul\n",
      "Train node 111 : features.5.2.stochastic_depth\n",
      "Train node 112 : features.5.2.add\n",
      "Train node 113 : features.5.3.features_5_3_layer_scale\n",
      "Train node 114 : features.5.3.block.0\n",
      "Train node 115 : features.5.3.block.1\n",
      "Train node 116 : features.5.3.block.2\n",
      "Train node 117 : features.5.3.block.3\n",
      "Train node 118 : features.5.3.block.4\n",
      "Train node 119 : features.5.3.block.5\n",
      "Train node 120 : features.5.3.block.6\n",
      "Train node 121 : features.5.3.mul\n",
      "Train node 122 : features.5.3.stochastic_depth\n",
      "Train node 123 : features.5.3.add\n",
      "Train node 124 : features.5.4.features_5_4_layer_scale\n",
      "Train node 125 : features.5.4.block.0\n",
      "Train node 126 : features.5.4.block.1\n",
      "Train node 127 : features.5.4.block.2\n",
      "Train node 128 : features.5.4.block.3\n",
      "Train node 129 : features.5.4.block.4\n",
      "Train node 130 : features.5.4.block.5\n",
      "Train node 131 : features.5.4.block.6\n",
      "Train node 132 : features.5.4.mul\n",
      "Train node 133 : features.5.4.stochastic_depth\n",
      "Train node 134 : features.5.4.add\n",
      "Train node 135 : features.5.5.features_5_5_layer_scale\n",
      "Train node 136 : features.5.5.block.0\n",
      "Train node 137 : features.5.5.block.1\n",
      "Train node 138 : features.5.5.block.2\n",
      "Train node 139 : features.5.5.block.3\n",
      "Train node 140 : features.5.5.block.4\n",
      "Train node 141 : features.5.5.block.5\n",
      "Train node 142 : features.5.5.block.6\n",
      "Train node 143 : features.5.5.mul\n",
      "Train node 144 : features.5.5.stochastic_depth\n",
      "Train node 145 : features.5.5.add\n",
      "Train node 146 : features.5.6.features_5_6_layer_scale\n",
      "Train node 147 : features.5.6.block.0\n",
      "Train node 148 : features.5.6.block.1\n",
      "Train node 149 : features.5.6.block.2\n",
      "Train node 150 : features.5.6.block.3\n",
      "Train node 151 : features.5.6.block.4\n",
      "Train node 152 : features.5.6.block.5\n",
      "Train node 153 : features.5.6.block.6\n",
      "Train node 154 : features.5.6.mul\n",
      "Train node 155 : features.5.6.stochastic_depth\n",
      "Train node 156 : features.5.6.add\n",
      "Train node 157 : features.5.7.features_5_7_layer_scale\n",
      "Train node 158 : features.5.7.block.0\n",
      "Train node 159 : features.5.7.block.1\n",
      "Train node 160 : features.5.7.block.2\n",
      "Train node 161 : features.5.7.block.3\n",
      "Train node 162 : features.5.7.block.4\n",
      "Train node 163 : features.5.7.block.5\n",
      "Train node 164 : features.5.7.block.6\n",
      "Train node 165 : features.5.7.mul\n",
      "Train node 166 : features.5.7.stochastic_depth\n",
      "Train node 167 : features.5.7.add\n",
      "Train node 168 : features.5.8.features_5_8_layer_scale\n",
      "Train node 169 : features.5.8.block.0\n",
      "Train node 170 : features.5.8.block.1\n",
      "Train node 171 : features.5.8.block.2\n",
      "Train node 172 : features.5.8.block.3\n",
      "Train node 173 : features.5.8.block.4\n",
      "Train node 174 : features.5.8.block.5\n",
      "Train node 175 : features.5.8.block.6\n",
      "Train node 176 : features.5.8.mul\n",
      "Train node 177 : features.5.8.stochastic_depth\n",
      "Train node 178 : features.5.8.add\n",
      "Train node 179 : features.6.0.permute\n",
      "Train node 180 : features.6.0.features_6_0_weight\n",
      "Train node 181 : features.6.0.features_6_0_bias\n",
      "Train node 182 : features.6.0.layer_norm\n",
      "Train node 183 : features.6.0.permute_1\n",
      "Train node 184 : features.6.1\n",
      "Train node 185 : features.7.0.features_7_0_layer_scale\n",
      "Train node 186 : features.7.0.block.0\n",
      "Train node 187 : features.7.0.block.1\n",
      "Train node 188 : features.7.0.block.2\n",
      "Train node 189 : features.7.0.block.3\n",
      "Train node 190 : features.7.0.block.4\n",
      "Train node 191 : features.7.0.block.5\n",
      "Train node 192 : features.7.0.block.6\n",
      "Train node 193 : features.7.0.mul\n",
      "Train node 194 : features.7.0.stochastic_depth\n",
      "Train node 195 : features.7.0.add\n",
      "Train node 196 : features.7.1.features_7_1_layer_scale\n",
      "Train node 197 : features.7.1.block.0\n",
      "Train node 198 : features.7.1.block.1\n",
      "Train node 199 : features.7.1.block.2\n",
      "Train node 200 : features.7.1.block.3\n",
      "Train node 201 : features.7.1.block.4\n",
      "Train node 202 : features.7.1.block.5\n",
      "Train node 203 : features.7.1.block.6\n",
      "Train node 204 : features.7.1.mul\n",
      "Train node 205 : features.7.1.stochastic_depth\n",
      "Train node 206 : features.7.1.add\n",
      "Train node 207 : features.7.2.features_7_2_layer_scale\n",
      "Train node 208 : features.7.2.block.0\n",
      "Train node 209 : features.7.2.block.1\n",
      "Train node 210 : features.7.2.block.2\n",
      "Train node 211 : features.7.2.block.3\n",
      "Train node 212 : features.7.2.block.4\n",
      "Train node 213 : features.7.2.block.5\n",
      "Train node 214 : features.7.2.block.6\n",
      "Train node 215 : features.7.2.mul\n",
      "Train node 216 : features.7.2.stochastic_depth\n",
      "Train node 217 : features.7.2.add\n",
      "Train node 218 : avgpool\n",
      "Train node 219 : classifier.0.permute\n",
      "Train node 220 : classifier.0.classifier_0_weight\n",
      "Train node 221 : classifier.0.classifier_0_bias\n",
      "Train node 222 : classifier.0.layer_norm\n",
      "Train node 223 : classifier.0.permute_1\n",
      "Train node 224 : classifier.1\n",
      "Train node 225 : classifier.2\n",
      "##############THE NODES TO BE SELECTED FROM THE CONVNEXT MODEL#############################\n",
      "Train node 9 : features.1.0.block.6\n",
      "Train node 48 : features.3.0.block.6\n",
      "Train node 87 : features.5.0.block.6\n",
      "Train node 120 : features.5.3.block.6\n",
      "Train node 153 : features.5.6.block.6\n",
      "Train node 175 : features.5.8.block.6\n",
      "Train node 192 : features.7.0.block.6\n",
      "Train node 225 : classifier.2\n",
      "##########################PRINTING THE SHAPES OF THE FEATURES TO BE EXRACTED###############\n",
      "torch.Size([301056])\n",
      "torch.Size([150528])\n",
      "torch.Size([75264])\n",
      "torch.Size([75264])\n",
      "torch.Size([75264])\n",
      "torch.Size([75264])\n",
      "torch.Size([37632])\n",
      "torch.Size([1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "convnext = convnext_tiny(weights=convnext_weights)\n",
    "\n",
    "convnext_no_fc= torch.nn.Sequential(*list(convnext.children())[:-1]) # strips off last linear layer\n",
    "#print(convnext_no_fc)\n",
    "preprocess=convnext_weights.transforms()\n",
    "img=read_image(\"/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/test/ILSVRC2012_test_00000046.JPEG\")\n",
    "batch=preprocess(img).unsqueeze(0)\n",
    "\n",
    "prediction=convnext_no_fc(batch)\n",
    "# Convert pytorch model to flow of data diagram\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "model_dia=make_dot(convnext(batch),params=dict(convnext_no_fc.named_parameters()), show_attrs=True, show_saved=True)\n",
    "model_dia.format=\"png\"\n",
    "model_dia.render(\"convnext_model_dia\")\n",
    "print(\"#####################SHOW THE MODEL NODES##############\")\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "train_nodes, eval_nodes = get_graph_node_names(convnext)\n",
    "for i,j in enumerate(train_nodes):\n",
    "    print(f\"Train node {i} : {j}\")\n",
    "    \n",
    "print(\"##############THE NODES TO BE SELECTED FROM THE CONVNEXT MODEL#############################\")\n",
    "list_of_layers_index=[9,48,87,120,153,175,192,225]\n",
    "layers_to_be_printed=dict()\n",
    "for i,j in enumerate(train_nodes):\n",
    "    if i in list_of_layers_index:\n",
    "        print(f\"Train node {i} : {j}\")\n",
    "        layers_to_be_printed[j]=f\"Output Layer {i}\"\n",
    "convnext_return_nodes=layers_to_be_printed\n",
    "convnext_feature_extractor=create_feature_extractor(convnext,return_nodes=convnext_return_nodes)\n",
    "img=read_image(\"/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/test/ILSVRC2012_test_00000046.JPEG\")\n",
    "batch=preprocess(img).unsqueeze(0)\n",
    "feature_list=convnext_feature_extractor(batch)\n",
    "print(\"##########################PRINTING THE SHAPES OF THE FEATURES TO BE EXRACTED###############\")\n",
    "for x in feature_list:\n",
    "    print(torch.flatten(feature_list[x]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e709260",
   "metadata": {
    "papermill": {
     "duration": 0.010347,
     "end_time": "2023-11-13T00:34:59.831788",
     "exception": false,
     "start_time": "2023-11-13T00:34:59.821441",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dd49d74",
   "metadata": {
    "papermill": {
     "duration": 0.010333,
     "end_time": "2023-11-13T00:34:59.852858",
     "exception": false,
     "start_time": "2023-11-13T00:34:59.842525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Now features extraction from AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdb4fed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:34:59.879670Z",
     "iopub.status.busy": "2023-11-13T00:34:59.879113Z",
     "iopub.status.idle": "2023-11-13T00:35:02.359397Z",
     "shell.execute_reply": "2023-11-13T00:35:02.358035Z"
    },
    "papermill": {
     "duration": 2.496689,
     "end_time": "2023-11-13T00:35:02.362146",
     "exception": false,
     "start_time": "2023-11-13T00:34:59.865457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "100%|██████████| 233M/233M [00:01<00:00, 193MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "African elephant : 75.67582726478577 % \n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import alexnet,AlexNet_Weights\n",
    "alexnet_weights=AlexNet_Weights.IMAGENET1K_V1\n",
    "model=alexnet(weights=alexnet_weights)\n",
    "#print(model)\n",
    "model.eval()\n",
    "\n",
    "preprocess=alexnet_weights.transforms()\n",
    "img=read_image(\"/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/test/ILSVRC2012_test_00000046.JPEG\")\n",
    "batch=preprocess(img).unsqueeze(0)\n",
    "\n",
    "prediction=model(batch).squeeze(0).softmax(0)\n",
    "class_id= prediction.argmax().item()\n",
    "score=prediction[class_id].item()\n",
    "category_name=alexnet_weights.meta[\"categories\"][class_id]\n",
    "print(f\"{category_name} : {score*100} % \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b02b118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:35:02.391809Z",
     "iopub.status.busy": "2023-11-13T00:35:02.390572Z",
     "iopub.status.idle": "2023-11-13T00:35:03.359051Z",
     "shell.execute_reply": "2023-11-13T00:35:03.357796Z"
    },
    "papermill": {
     "duration": 0.985292,
     "end_time": "2023-11-13T00:35:03.362603",
     "exception": false,
     "start_time": "2023-11-13T00:35:02.377311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "alexnet_model=alexnet(weights=alexnet_weights)\n",
    "\n",
    "print(alexnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c280e6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:35:03.389766Z",
     "iopub.status.busy": "2023-11-13T00:35:03.388803Z",
     "iopub.status.idle": "2023-11-13T00:35:03.477031Z",
     "shell.execute_reply": "2023-11-13T00:35:03.476010Z"
    },
    "papermill": {
     "duration": 0.103456,
     "end_time": "2023-11-13T00:35:03.479006",
     "exception": false,
     "start_time": "2023-11-13T00:35:03.375550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "preprocess=alexnet_weights.transforms()\n",
    "img=read_image(\"/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/test/ILSVRC2012_test_00000046.JPEG\")\n",
    "batch=preprocess(img).unsqueeze(0)\n",
    "print(batch.shape)\n",
    "prediction=alexnet_model(batch)\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "model_dia=make_dot(alexnet_model(batch),params=dict(alexnet_model.named_parameters()), show_attrs=True, show_saved=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00882cda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:35:03.504857Z",
     "iopub.status.busy": "2023-11-13T00:35:03.504176Z",
     "iopub.status.idle": "2023-11-13T00:35:04.033287Z",
     "shell.execute_reply": "2023-11-13T00:35:04.031877Z"
    },
    "papermill": {
     "duration": 0.546325,
     "end_time": "2023-11-13T00:35:04.037107",
     "exception": false,
     "start_time": "2023-11-13T00:35:03.490782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alexnet_model_dia.png'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dia.format=\"png\"\n",
    "model_dia.render(\"alexnet_model_dia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb76002a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:35:04.068930Z",
     "iopub.status.busy": "2023-11-13T00:35:04.068343Z",
     "iopub.status.idle": "2023-11-13T00:35:04.092823Z",
     "shell.execute_reply": "2023-11-13T00:35:04.091573Z"
    },
    "papermill": {
     "duration": 0.045768,
     "end_time": "2023-11-13T00:35:04.095621",
     "exception": false,
     "start_time": "2023-11-13T00:35:04.049853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "train_nodes, eval_nodes = get_graph_node_names(alexnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "841b5f21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:35:04.126488Z",
     "iopub.status.busy": "2023-11-13T00:35:04.126015Z",
     "iopub.status.idle": "2023-11-13T00:35:04.130901Z",
     "shell.execute_reply": "2023-11-13T00:35:04.129850Z"
    },
    "papermill": {
     "duration": 0.023372,
     "end_time": "2023-11-13T00:35:04.132840",
     "exception": false,
     "start_time": "2023-11-13T00:35:04.109468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_layers_index=[1,4,7,9,11,17,20,22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62c8c8d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:35:04.159060Z",
     "iopub.status.busy": "2023-11-13T00:35:04.157987Z",
     "iopub.status.idle": "2023-11-13T00:35:04.166246Z",
     "shell.execute_reply": "2023-11-13T00:35:04.165403Z"
    },
    "papermill": {
     "duration": 0.023892,
     "end_time": "2023-11-13T00:35:04.168733",
     "exception": false,
     "start_time": "2023-11-13T00:35:04.144841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train node 0 : x\n",
      "Train node 1 : features.0\n",
      "Train node 2 : features.1\n",
      "Train node 3 : features.2\n",
      "Train node 4 : features.3\n",
      "Train node 5 : features.4\n",
      "Train node 6 : features.5\n",
      "Train node 7 : features.6\n",
      "Train node 8 : features.7\n",
      "Train node 9 : features.8\n",
      "Train node 10 : features.9\n",
      "Train node 11 : features.10\n",
      "Train node 12 : features.11\n",
      "Train node 13 : features.12\n",
      "Train node 14 : avgpool\n",
      "Train node 15 : flatten\n",
      "Train node 16 : classifier.0\n",
      "Train node 17 : classifier.1\n",
      "Train node 18 : classifier.2\n",
      "Train node 19 : classifier.3\n",
      "Train node 20 : classifier.4\n",
      "Train node 21 : classifier.5\n",
      "Train node 22 : classifier.6\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(train_nodes):\n",
    "    print(f\"Train node {i} : {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d094d0af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:35:04.197722Z",
     "iopub.status.busy": "2023-11-13T00:35:04.196662Z",
     "iopub.status.idle": "2023-11-13T00:35:04.202911Z",
     "shell.execute_reply": "2023-11-13T00:35:04.201651Z"
    },
    "papermill": {
     "duration": 0.023251,
     "end_time": "2023-11-13T00:35:04.205000",
     "exception": false,
     "start_time": "2023-11-13T00:35:04.181749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train node 1 : features.0\n",
      "Train node 4 : features.3\n",
      "Train node 7 : features.6\n",
      "Train node 9 : features.8\n",
      "Train node 11 : features.10\n",
      "Train node 17 : classifier.1\n",
      "Train node 20 : classifier.4\n",
      "Train node 22 : classifier.6\n",
      "{'features.0': 'Output Layer 1', 'features.3': 'Output Layer 4', 'features.6': 'Output Layer 7', 'features.8': 'Output Layer 9', 'features.10': 'Output Layer 11', 'classifier.1': 'Output Layer 17', 'classifier.4': 'Output Layer 20', 'classifier.6': 'Output Layer 22'}\n"
     ]
    }
   ],
   "source": [
    "layers_to_be_printed=dict()\n",
    "for i,j in enumerate(train_nodes):\n",
    "    if i in list_of_layers_index:\n",
    "        print(f\"Train node {i} : {j}\")\n",
    "        layers_to_be_printed[j]=f\"Output Layer {i}\"\n",
    "print(layers_to_be_printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab5d4f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:35:04.290940Z",
     "iopub.status.busy": "2023-11-13T00:35:04.290268Z",
     "iopub.status.idle": "2023-11-13T00:35:04.362911Z",
     "shell.execute_reply": "2023-11-13T00:35:04.361656Z"
    },
    "papermill": {
     "duration": 0.090713,
     "end_time": "2023-11-13T00:35:04.365740",
     "exception": false,
     "start_time": "2023-11-13T00:35:04.275027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([193600])\n",
      "torch.Size([139968])\n",
      "torch.Size([64896])\n",
      "torch.Size([43264])\n",
      "torch.Size([43264])\n",
      "torch.Size([4096])\n",
      "torch.Size([4096])\n",
      "torch.Size([1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "alexnet_return_nodes=layers_to_be_printed\n",
    "\n",
    "alexnet_feature_extractor=create_feature_extractor(alexnet_model,return_nodes=alexnet_return_nodes)\n",
    "img=read_image(\"/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/test/ILSVRC2012_test_00000046.JPEG\")\n",
    "batch=preprocess(img).unsqueeze(0)\n",
    "feature_list=alexnet_feature_extractor(batch)\n",
    "\n",
    "for x in feature_list:\n",
    "    print(torch.flatten(feature_list[x]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b3e88",
   "metadata": {
    "papermill": {
     "duration": 0.012318,
     "end_time": "2023-11-13T00:35:04.390589",
     "exception": false,
     "start_time": "2023-11-13T00:35:04.378271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Now looping through all images used in the seen vs imagined study to regenrate the features and save them as files for decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ecaa3aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:35:04.417754Z",
     "iopub.status.busy": "2023-11-13T00:35:04.416904Z",
     "iopub.status.idle": "2023-11-13T00:35:04.422899Z",
     "shell.execute_reply": "2023-11-13T00:35:04.421830Z"
    },
    "papermill": {
     "duration": 0.021789,
     "end_time": "2023-11-13T00:35:04.424816",
     "exception": false,
     "start_time": "2023-11-13T00:35:04.403027",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import requests\n",
    "# import torchvision.transforms.functional as TF\n",
    "# count=0\n",
    "# import pandas as pd \n",
    "# df_url=pd.read_csv(\"/kaggle/input/seen-vs-imagined-dataset-project/imageNet_URLTraining_fixed.csv\",header=None).astype(\"str\").values.tolist()\n",
    "# imagenet_train_loc=\"/kaggle/input/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/train/\"\n",
    "# for i,j in enumerate(df_url):\n",
    "    \n",
    "   \n",
    "#     try:\n",
    "#         image_file_path=imagenet_train_loc+f\"n0{j}/\"+df_url[i][2]\n",
    "#         url = df_url[i][0]\n",
    "#         #print(url)\n",
    "#         try:\n",
    "#             img = Image.open(requests.get(url, stream=True).raw)\n",
    "#             print(f\"URL ALL GOOD for : count:{i} file_ame:{df_url[i][1]}\")\n",
    "#         except:\n",
    "#             print(f\"URL:{df_url[i][0]} not opening trying for imagenet\")\n",
    "#             try:\n",
    "#                 img=Image.open(image_file_path)\n",
    "#                 print(\"file opened from imagenet\")\n",
    "#             except:\n",
    "#                 print(f\"image {df_url[i][1]} found online or in imagenet\")\n",
    "#         count+=1\n",
    "#     except:\n",
    "#         print(f\"{df_url[i][1]} file not found\")\n",
    "#     print(img.size)\n",
    "# print(f\"correctly loaded files are {count}/{len(df_url)}\")\n",
    "#         #print(\"ERROR at finding file name:\",count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba343a75",
   "metadata": {
    "papermill": {
     "duration": 0.011922,
     "end_time": "2023-11-13T00:35:04.449524",
     "exception": false,
     "start_time": "2023-11-13T00:35:04.437602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# code for data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96b5435f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:35:04.479534Z",
     "iopub.status.busy": "2023-11-13T00:35:04.478882Z",
     "iopub.status.idle": "2023-11-13T00:35:04.486540Z",
     "shell.execute_reply": "2023-11-13T00:35:04.485801Z"
    },
    "papermill": {
     "duration": 0.026267,
     "end_time": "2023-11-13T00:35:04.488544",
     "exception": false,
     "start_time": "2023-11-13T00:35:04.462277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import torchvision.io\n",
    "class IMG_Dataset(Dataset):\n",
    "    def __init__(self, image_dir, transform = None):\n",
    "        self.image_dir = image_dir\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        \n",
    "        image = read_image(img_path,mode=torchvision.io.ImageReadMode.RGB)\n",
    "\n",
    "\n",
    "        if self.transform is  not None:\n",
    "            try:\n",
    "                transformed_img= self.transform(image)\n",
    "                image = transformed_img\n",
    "            except:\n",
    "                print(f\"error found at file {img_path}\")\n",
    "                print(image.shape)\n",
    "\n",
    "        return image,img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c56b19be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:35:04.515312Z",
     "iopub.status.busy": "2023-11-13T00:35:04.514699Z",
     "iopub.status.idle": "2023-11-13T00:35:04.520360Z",
     "shell.execute_reply": "2023-11-13T00:35:04.519635Z"
    },
    "papermill": {
     "duration": 0.02124,
     "end_time": "2023-11-13T00:35:04.522222",
     "exception": false,
     "start_time": "2023-11-13T00:35:04.500982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_loaders(\n",
    "        train_dir,\n",
    "        val_dir,\n",
    "        batch_size,\n",
    "        train_transform,\n",
    "        test_transform,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "):\n",
    "    train_dataset = IMG_Dataset(\n",
    "        image_dir=train_dir,\n",
    "        transform=train_transform\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_dataset = IMG_Dataset(\n",
    "        image_dir=val_dir,\n",
    "        transform=test_transform\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b2135a",
   "metadata": {
    "papermill": {
     "duration": 0.012777,
     "end_time": "2023-11-13T00:35:04.547960",
     "exception": false,
     "start_time": "2023-11-13T00:35:04.535183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Alexnet features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9972643f",
   "metadata": {
    "papermill": {
     "duration": 0.014531,
     "end_time": "2023-11-13T00:35:04.575227",
     "exception": false,
     "start_time": "2023-11-13T00:35:04.560696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# for training data imgs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c66459b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:35:04.602463Z",
     "iopub.status.busy": "2023-11-13T00:35:04.601830Z",
     "iopub.status.idle": "2023-11-13T00:35:04.740283Z",
     "shell.execute_reply": "2023-11-13T00:35:04.739188Z"
    },
    "papermill": {
     "duration": 0.154721,
     "end_time": "2023-11-13T00:35:04.742671",
     "exception": false,
     "start_time": "2023-11-13T00:35:04.587950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "alex_train_transform =alexnet_weights.transforms()\n",
    "alex_test_transforms =alexnet_weights.transforms()\n",
    "train_dir=\"/kaggle/input/seen-vs-imagined-horikawa-et-al-imagenet-subset/images/training\"\n",
    "test_dir=\"/kaggle/input/seen-vs-imagined-horikawa-et-al-imagenet-subset/images/test\"\n",
    "num_workers=2\n",
    "pin_memory=True\n",
    "batch_size=1\n",
    "device=\"cpu\"\n",
    "train_loader, test_loader = get_loaders(\n",
    "        train_dir,\n",
    "        test_dir,\n",
    "        batch_size,\n",
    "        alex_train_transform,\n",
    "        alex_test_transforms,\n",
    "        num_workers,\n",
    "        pin_memory,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa6b8403",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:35:04.773017Z",
     "iopub.status.busy": "2023-11-13T00:35:04.772271Z",
     "iopub.status.idle": "2023-11-13T00:36:17.714159Z",
     "shell.execute_reply": "2023-11-13T00:36:17.713011Z"
    },
    "papermill": {
     "duration": 72.961991,
     "end_time": "2023-11-13T00:36:17.717306",
     "exception": false,
     "start_time": "2023-11-13T00:35:04.755315",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [01:11<00:00, 16.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "alexnet_feature_dict=dict()\n",
    "loop = tqdm(train_loader)\n",
    "for batch_idx,(data,image_path)  in enumerate(loop):\n",
    "    \n",
    "    data = data.to(device=device)\n",
    "    feature_list=alexnet_feature_extractor(data)\n",
    "    flattened_feature_dict=dict()\n",
    "    for x in feature_list:\n",
    "        #print(feature_list[x].shape)\n",
    "        y=torch.flatten(feature_list[x]).detach().cpu().numpy()#flatten in code not working\n",
    "        sampled_size=1000\n",
    "        sampled_features=np.random.choice(y,size=(1000),replace=False)\n",
    "        flattened_feature_dict[x]=torch.unsqueeze(torch.from_numpy(sampled_features),0)\n",
    "        #print(flattened_feature_dict[x].shape)\n",
    "        \n",
    "    \n",
    "    img_name=str(*image_path).split(\"/\")[-1][1:-5]\n",
    "    alexnet_feature_dict[img_name]=flattened_feature_dict\n",
    "\n",
    "#     for x in feature_list:\n",
    "#         print(torch.flatten(feature_list[x]).shape)\n",
    "#print(alexnet_feature_dict)\n",
    "alex_train_fatures_dict_torch=TensorDict(alexnet_feature_dict,batch_size=1)\n",
    "torch.save(alex_train_fatures_dict_torch,\"alexnet_dict_train.pt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b600da0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:36:17.820220Z",
     "iopub.status.busy": "2023-11-13T00:36:17.818946Z",
     "iopub.status.idle": "2023-11-13T00:36:17.825948Z",
     "shell.execute_reply": "2023-11-13T00:36:17.824505Z"
    },
    "papermill": {
     "duration": 0.061201,
     "end_time": "2023-11-13T00:36:17.828411",
     "exception": false,
     "start_time": "2023-11-13T00:36:17.767210",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y =alex_train_fatures_dict_torch[\"01518878_10042\"][\"Output Layer 11\"].detach().cpu().numpy()\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7f330",
   "metadata": {
    "papermill": {
     "duration": 0.053367,
     "end_time": "2023-11-13T00:36:17.933741",
     "exception": false,
     "start_time": "2023-11-13T00:36:17.880374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# for test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad90d9c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:36:18.039446Z",
     "iopub.status.busy": "2023-11-13T00:36:18.038191Z",
     "iopub.status.idle": "2023-11-13T00:36:21.179067Z",
     "shell.execute_reply": "2023-11-13T00:36:21.177599Z"
    },
    "papermill": {
     "duration": 3.196626,
     "end_time": "2023-11-13T00:36:21.181921",
     "exception": false,
     "start_time": "2023-11-13T00:36:17.985295",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 16.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "alexnet_feature_dict_test=dict()\n",
    "loop = tqdm(test_loader)\n",
    "for batch_idx,(data,image_path)  in enumerate(loop):\n",
    "    #print(\"new img features\")\n",
    "    data = data.to(device=device)\n",
    "    feature_list=alexnet_feature_extractor(data)\n",
    "    flattened_feature_dict=dict()\n",
    "    for x in feature_list:\n",
    "        #print(feature_list[x].shape)\n",
    "        \n",
    "        y=torch.flatten(feature_list[x]).detach().cpu().numpy()#flatten in code not working\n",
    "        sampled_size=1000\n",
    "        sampled_features=np.random.choice(y,size=(1000),replace=False)\n",
    "        flattened_feature_dict[x]=torch.unsqueeze(torch.from_numpy(sampled_features),0)\n",
    "        #print(flattened_feature_dict[x].shape)\n",
    "    img_name=str(*image_path).split(\"/\")[-1][1:-5]\n",
    "    \n",
    "    alexnet_feature_dict_test[img_name]=flattened_feature_dict\n",
    "\n",
    "#     for x in feature_list:\n",
    "#         print(torch.flatten(feature_list[x]).shape)\n",
    "\n",
    "alex_train_fatures_dict_torch_test=TensorDict(alexnet_feature_dict_test,batch_size=1)\n",
    "torch.save(alex_train_fatures_dict_torch_test,\"alexnet_dict_test.pt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc80dbb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:36:21.286947Z",
     "iopub.status.busy": "2023-11-13T00:36:21.285661Z",
     "iopub.status.idle": "2023-11-13T00:36:21.291009Z",
     "shell.execute_reply": "2023-11-13T00:36:21.289921Z"
    },
    "papermill": {
     "duration": 0.059883,
     "end_time": "2023-11-13T00:36:21.293159",
     "exception": false,
     "start_time": "2023-11-13T00:36:21.233276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(alex_train_fatures_dict_torch_test)\n",
    "# x =alex_train_fatures_dict_torch_test[\"01443537_22563\"][\"Output Layer 11\"].detach().cpu().numpy()\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa9d94",
   "metadata": {
    "papermill": {
     "duration": 0.055207,
     "end_time": "2023-11-13T00:36:21.404335",
     "exception": false,
     "start_time": "2023-11-13T00:36:21.349128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# For convnext "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa0b7e",
   "metadata": {
    "papermill": {
     "duration": 0.053775,
     "end_time": "2023-11-13T00:36:21.527171",
     "exception": false,
     "start_time": "2023-11-13T00:36:21.473396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "299716dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:36:21.635581Z",
     "iopub.status.busy": "2023-11-13T00:36:21.635141Z",
     "iopub.status.idle": "2023-11-13T00:36:21.640374Z",
     "shell.execute_reply": "2023-11-13T00:36:21.639341Z"
    },
    "papermill": {
     "duration": 0.064002,
     "end_time": "2023-11-13T00:36:21.642895",
     "exception": false,
     "start_time": "2023-11-13T00:36:21.578893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/kaggle/working/convnext_train\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bb40a4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:36:21.763103Z",
     "iopub.status.busy": "2023-11-13T00:36:21.762655Z",
     "iopub.status.idle": "2023-11-13T00:39:27.665636Z",
     "shell.execute_reply": "2023-11-13T00:39:27.664467Z"
    },
    "papermill": {
     "duration": 185.96255,
     "end_time": "2023-11-13T00:39:27.668991",
     "exception": false,
     "start_time": "2023-11-13T00:36:21.706441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [03:05<00:00,  6.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "convnext_feature_dict_train=dict()\n",
    "loop = tqdm(train_loader)\n",
    "for batch_idx,(data,image_path)  in enumerate(loop):       \n",
    "    #print(\"new img features\")\n",
    "    data = data.to(device=device)\n",
    "    feature_list=convnext_feature_extractor(data)\n",
    "    flattened_feature_dict=dict()\n",
    "    for x in feature_list:\n",
    "        #print(feature_list[x].shape)\n",
    "        \n",
    "        y=torch.flatten(feature_list[x]).detach().cpu().numpy()#flatten in code not working\n",
    "        sampled_size=1000\n",
    "        sampled_features=np.random.choice(y,size=(1000),replace=False)\n",
    "        flattened_feature_dict[x]=torch.unsqueeze(torch.from_numpy(sampled_features),0)\n",
    "        #print(flattened_feature_dict[x].shape)\n",
    "    img_name=str(*image_path).split(\"/\")[-1][1:-5]\n",
    "    \n",
    "    convnext_feature_dict_train[img_name]=flattened_feature_dict\n",
    "#     for x in feature_list:\n",
    "#         print(torch.flatten(feature_list[x]).shape)\n",
    "\n",
    "convnext_train_fatures_dict_torch_train=TensorDict(convnext_feature_dict_train,batch_size=1)\n",
    "torch.save(convnext_train_fatures_dict_torch_train,\"convnext_features_train.pt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17279798",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:39:27.921168Z",
     "iopub.status.busy": "2023-11-13T00:39:27.919832Z",
     "iopub.status.idle": "2023-11-13T00:39:27.925128Z",
     "shell.execute_reply": "2023-11-13T00:39:27.924165Z"
    },
    "papermill": {
     "duration": 0.132842,
     "end_time": "2023-11-13T00:39:27.927131",
     "exception": false,
     "start_time": "2023-11-13T00:39:27.794289",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!zip -r \"conv_next_train.zip\" \"/kaggle/working/convnext_train/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2a5dae",
   "metadata": {
    "papermill": {
     "duration": 0.123276,
     "end_time": "2023-11-13T00:39:28.175341",
     "exception": false,
     "start_time": "2023-11-13T00:39:28.052065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# convnext test img features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a97e818",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:39:28.424004Z",
     "iopub.status.busy": "2023-11-13T00:39:28.423608Z",
     "iopub.status.idle": "2023-11-13T00:39:36.894542Z",
     "shell.execute_reply": "2023-11-13T00:39:36.893187Z"
    },
    "papermill": {
     "duration": 8.596511,
     "end_time": "2023-11-13T00:39:36.896959",
     "exception": false,
     "start_time": "2023-11-13T00:39:28.300448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:08<00:00,  5.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "convnext_feature_dict_test=dict()\n",
    "loop = tqdm(test_loader)\n",
    "for batch_idx,(data,image_path)  in enumerate(loop):\n",
    "    \n",
    "    data = data.to(device=device)\n",
    "    feature_list=convnext_feature_extractor(data)\n",
    "    flattened_feature_dict=dict()\n",
    "    for x in feature_list:\n",
    "        #print(feature_list[x].shape)\n",
    "        \n",
    "        y=torch.flatten(feature_list[x]).detach().cpu().numpy()#flatten in code not working\n",
    "        sampled_size=1000\n",
    "        sampled_features=np.random.choice(y,size=(1000),replace=False)\n",
    "        flattened_feature_dict[x]=torch.unsqueeze(torch.from_numpy(sampled_features),0)\n",
    "        #print(flattened_feature_dict[x].shape)\n",
    "    img_name=str(*image_path).split(\"/\")[-1][1:-5]\n",
    "    convnext_feature_dict_test[img_name]=flattened_feature_dict\n",
    "#     for x in feature_list:\n",
    "#         print(torch.flatten(feature_list[x]).shape)\n",
    "\n",
    "convnext_train_fatures_dict_torch_test=TensorDict(convnext_feature_dict_test,batch_size=1)\n",
    "torch.save(convnext_train_fatures_dict_torch_test,\"convnext_features_test.pt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a393dc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-13T00:39:37.148889Z",
     "iopub.status.busy": "2023-11-13T00:39:37.147900Z",
     "iopub.status.idle": "2023-11-13T00:39:42.561265Z",
     "shell.execute_reply": "2023-11-13T00:39:42.560173Z"
    },
    "papermill": {
     "duration": 5.542329,
     "end_time": "2023-11-13T00:39:42.563804",
     "exception": false,
     "start_time": "2023-11-13T00:39:37.021475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/alexnet_dict_test.pt (deflated 65%)\r\n",
      "  adding: kaggle/working/alexnet_dict_train.pt (deflated 65%)\r\n",
      "  adding: kaggle/working/convnext_features_test.pt (deflated 11%)\r\n",
      "  adding: kaggle/working/convnext_features_train.pt (deflated 12%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r \"test_train_img_features.zip\" \"/kaggle/working/alexnet_dict_test.pt\" \"/kaggle/working/alexnet_dict_train.pt\" \"/kaggle/working/convnext_features_test.pt\" \"/kaggle/working/convnext_features_train.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9ab94",
   "metadata": {
    "papermill": {
     "duration": 0.12306,
     "end_time": "2023-11-13T00:39:42.811120",
     "exception": false,
     "start_time": "2023-11-13T00:39:42.688060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 336.726194,
   "end_time": "2023-11-13T00:39:44.258037",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-13T00:34:07.531843",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
