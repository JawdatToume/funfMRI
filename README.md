# funfMRI

This is the code we used to generate the results by testing and training our models for our CMPUT 624 project.

We have code used for 3 different model-building techniques, the TDA, baseline, and our CNN.

Firstly, we have code used to collect our fMRI data, to extract it, load it, mask it, and prepare it to be used by our other files of code to perform our training and evaluation. loader.py is used for loading, masker.py is used for masking. These files are mostly intermediary files called by other functions.

In addition, we have regresser.py, which is used by each of the models to develop a linear regression model, this file generates the Sparse Linear Regression models, as well as labeler.py, which loads and labels our feature data for use in training and evaluation.

Our baseline is generated through roiaverager.py, typically, the main() function is adjusted to alter the different tests that are done or models that are trained for the baseline, each training run generates a unique pkl file to save time on training in the future, and then testing is handled within the file as well, restoring the pkl file if it already exists. Lines 38 (for training) and 42 (for testing) are of special interest, as those function calls determine which subject we are testing on, what layer we are checking for, and what particular experiments we are focusing on.


Each of the tests creates a csv which is used to generate barplot figures to show our results. plotgraphs.py is a file we use to generate these graphs, using plotgraphs.py [file] creates a barplot out of the data in [file].

## AlexNet and ConvNext-tiny image feature prediction

Preprocessed Image Features generated by us can be downloaded from:  [Img Features Drive Link](https://drive.google.com/drive/folders/1u3ZibkBIougeTN30zn0_p1VGc_S0RkLi?usp=sharing)

The Image features were generated using the [notebooks](https://github.com/JawdatToume/funfMRI/tree/main/python_notebooks) in the python_notebooks folder in the repository.

The code in [Modified Kamitani Lab Generic Object Decoding](https://github.com/JawdatToume/funfMRI/blob/main/kamitani%20generic%20obj%20decoding_code_modified_for_convnext.zip) has been for most of the image feature prediction training and analysis of the AlexNet Image Features regenerated and ConvNext-tiny Image features.

## Topological Data Analysis

The notebook run_tda.ipynb contains all requirement to convert, train, and test models using TDA. The specific pieces of code used are:

convertTDA(fileType="perceptionTraining") or convertTDA(fileType="perceptionTest")
- To convert the fMRI data to objects containing rips diagrams

convertAveragedTDA(fileType="perceptionTest")
- Similar to convertTDA but will average over repeated stimulus
- Can be called with perceptionTraining but this will not do much as the data have few repeats

joinTDA([1, 2], fileType="perceptionTest", prePI=None, save=False, suffix="CORR")
- returns a joined TDA object using the specified fileType of data for the subjects in the list ([1,2] in this case)
- prePI is used when testing to give a previously fit persistence imager object to use
- Save can be used to join and fit PIs in order to retain data for future use
- Original rips diagrams are removed to save space

TopologicalfMRI.getData()
- returns data and labels as a tuple (data, labels) for training or testing, data will be flattened and in a matrix form

from labeler import labelsToFeatures
labelsToFeatures(labels, layer=22, labelType="train")
- takes a list of labels, the layer to extract features from, and where labels come from (train or test)
- returns the n x 1000 matrix of labels


